<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hao Wang (ÁéãË±™)</title>

  <meta name="author" content="Hao Wang (ÁéãË±™)">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

  <!-- Font Awesome and Academicons for professional icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.4/css/academicons.min.css">

  <!-- Busuanzi Counter Script -->
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  <!-- Copy E-mail Script -->
  <script>
    function copyEmail(event) {
      event.preventDefault();
      const email = 'wanghao9610@gmail.com';

      // Try to use modern API
      if (navigator.clipboard && navigator.clipboard.writeText) {
        navigator.clipboard.writeText(email).then(function () {
          alert('E-mail address copied to clipboard: ' + email);
        }).catch(function () {
          // If failed, try to open email client
          window.location.href = 'mailto:' + email;
        });
      } else {
        // Fallback: directly open email client
        window.location.href = 'mailto:' + email;
      }
    }
  </script>

  <!-- Copy WeChat Script -->
  <script>
    function copyWechat(event) {
      event.preventDefault();
      const wechat = 'wangh9610';

      // Try to use modern API
      if (navigator.clipboard && navigator.clipboard.writeText) {
        navigator.clipboard.writeText(wechat).then(function () {
          alert('WeChat ID copied to clipboard: ' + wechat);
        }).catch(function () {
          // If failed, try to open wechat client
          window.location.href = 'wechat:' + wechat;
        });
      } else {
        // Fallback: directly open wechat client
        window.location.href = 'wechat:' + wechat;
      }
    }
  </script>

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Hao Wang (ÁéãË±™)
                  </p>
                  <p>
                    I'm currently a Ph.D. student at <a href="https://www.sysu-hcp.net/">HCP Lab</a>, SYSU, and <a
                      href="https://www.pcl.ac.cn/">Pengcheng Laboratory</a>, supervised
                    by
                    <a href="https://scholar.google.com/citations?user=voxznZAAAAAJ&hl">Prof. Xiaodan Liang</a> and <a
                      href="https://scholar.google.com/citations?user=c3iwWRcAAAAJ&hl">Associate Prof. Xiangyuan
                      Lan</a>. Before
                    that, I received my Master's degree from <a href="http://www.ia.cas.cn/">CASIA</a>, supervised by <a
                      href="https://scholar.google.com/citations?user=sOI-S7oAAAAJ&hl">Prof. Jing Liu</a>, and my
                    Bachelor's degree from <a href="https://www.bjtu.edu.cn/">BJTU</a>.
                    <br><br>
                    My research interests include:
                  <ul>
                    <li>Open-ended computer vision</li>
                    <li>Multimodal large language models</li>
                  </ul>
                  <strong>Open source promotes the development of technology.</strong>
                  <br><br>
                  <!-- I anticipate graduating in 2026 for industrial research positions. If you're interested, please feel
                  free to reach out to me via E-mail or WeChat. -->
                  <em style="font-style: italic;">I'm currently looking for collaborations, feel free to contact me via
                    E-mail or WeChat.</em>
                  </p>
                  <p style="text-align:center">
                    <a href="https://wanghao9610.github.io/"><i class="fas fa-home"></i> Home</a> &nbsp;/&nbsp;
                    <a href="mailto:wanghao9610@gmail.com" onclick="copyEmail(event)" style="cursor:pointer"><i
                        class="fas fa-envelope"></i> E-mail</a> &nbsp;/&nbsp;
                    <a href="wechat://wangh9610" onclick="copyWechat(event)" style="cursor:pointer"><i
                        class="fab fa-weixin"></i> WeChat</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=PZajgHYAAAAJ&h"><i
                        class="ai ai-google-scholar"></i> Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/wanghao9610/"><i class="fab fa-github"></i> Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <a href="images/haowang.jpg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/haowang.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- News -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>‚ú® News</h2>
                  <ul>
                    <li>[<strong>2025.11</strong>] Happy to announce that our paper <a
                        href="https://wanghao9610.github.io/X-SAM/">X-SAM</a> is accepted by AAAI 2026.</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Publications -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>üìë Publications</h2>
                  <!-- <p>
                    Some papers are <span class="highlight">highlighted</span>.
                  </p> -->
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <!-- X-SAM -->
            <tbody>
              <tr onmouseout="xsam_stop()" onmouseover="xsam_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='xsam_image'>
                      <img src='images/xsam_after.jpg' width=100%>
                    </div>
                    <img src='images/xsam_before.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function xsam_start() {
                      document.getElementById('xsam_image').style.opacity = "1";
                    }

                    function xsam_stop() {
                      document.getElementById('xsam_image').style.opacity = "0";
                    }
                    xsam_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://wanghao9610.github.io/X-SAM/">
                    <span class="papertitle">X-SAM: From Segment Anything to Any Segmentation</span>
                  </a>
                  <br>
                  <strong>Hao Wang</strong>,
                  <a href="https://scholar.google.com/citations?user=3PFZAg0AAAAJ&hl">Limeng Qiao</a>,
                  <a href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl">Zequn Jie</a>,
                  <a href="https://zhijian11.github.io/">Zhijian Huang</a>,
                  <a href="https://fcjian.github.io/">Chengjian Feng</a>,
                  <a href="https://openreview.net/profile?id=%7EZheng_Qingfang1">Qingfang Zheng</a>,
                  <a href="https://forestlinma.com/">Lin Ma</a>,
                  <a href="https://scholar.google.com/citations?user=c3iwWRcAAAAJ&hl">Xiangyuan Lan</a>,
                  <a href="https://scholar.google.com/citations?user=voxznZAAAAAJ&hl">Xiaodan Liang</a>,
                  <br>
                  <em>AAAI</em>, 2026
                  <br>
                  <a href="https://wanghao9610.github.io/X-SAM/">Project</a>
                  /
                  <a href="https://arxiv.org/abs/2508.04655">Paper</a>
                  /
                  <a href="https://github.com/wanghao9610/X-SAM">
                    <img
                      src="https://img.shields.io/github/stars/wanghao9610/X-SAM?style=flat-square&logo=github&logoColor=white&label=Code&color=black&cacheSeconds=86400"
                      alt="GitHub stars">
                  </a>
                  <p>
                    A novel unified multimodal large language model (MLLM) framework, which extends the segmentation
                    from segment
                    anything to any segmentation, enhancing pixel-level perceptual understanding.
                  </p>
                </td>
              </tr>
            </tbody>

            <!-- OV-DINO -->
            <tbody>
              <tr onmouseout="ovdino_stop()" onmouseover="ovdino_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='ovdino_image'>
                      <img src='images/ovdino_after.jpg' width=100%>
                    </div>
                    <img src='images/ovdino_before.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function ovdino_start() {
                      document.getElementById('ovdino_image').style.opacity = "1";
                    }

                    function ovdino_stop() {
                      document.getElementById('ovdino_image').style.opacity = "0";
                    }
                    ovdino_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://wanghao9610.github.io/OV-DINO/">
                    <span class="papertitle">OV-DINO: Unified Open-Vocabulary Detection with Language-Aware Selective
                      Fusion</span>
                  </a>
                  <br>
                  <strong>Hao Wang</strong>,
                  <a href="https://scholar.google.com/citations?user=yVxSn70AAAAJ&hl">Pengzhen Ren</a>,
                  <a href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl">Zequn Jie</a>,
                  <a href="https://scholar.google.com.sg/citations?user=jXLkbw8AAAAJ&hl">Xiao Dong</a>,
                  <a href="https://fcjian.github.io/">Chengjian Feng</a>,
                  <a href="https://scholar.google.com/citations?user=8tPN5CAAAAAJ&hl">Yinlong Qian</a>,
                  <a href="https://forestlinma.com/">Lin Ma</a>,
                  <a href="https://scholar.google.com/citations?user=Awsue7sAAAAJ&hl">Dongmei Jiang</a>,
                  <a href="https://scholar.google.com/citations?user=o_DllmIAAAAJ&hl">Yaowei Wang</a>,
                  <a href="https://scholar.google.com/citations?user=c3iwWRcAAAAJ&hl">Xiangyuan Lan</a>,
                  <a href="https://scholar.google.com/citations?user=voxznZAAAAAJ&hl">Xiaodan Liang</a>,
                  <br>
                  <em>arXiv Preprint</em>, 2024
                  <br>
                  <a href="https://wanghao9610.github.io/OV-DINO/">Project</a>
                  /
                  <a href="https://arxiv.org/abs/2407.07844">Paper</a>
                  /
                  <a href="https://github.com/wanghao9610/OV-DINO">
                    <img
                      src="https://img.shields.io/github/stars/wanghao9610/OV-DINO?style=flat-square&logo=github&logoColor=white&label=Code&color=black&cacheSeconds=86400"
                      alt="GitHub stars">
                  </a>
                  <p>
                    A novel unified open-vocabulary detection method, which is pre-trained on
                    diverse
                    large-scale datasets with language-aware selective fusion in a unified
                    framework.
                  </p>
                </td>
              </tr>
            </tbody>

            <!-- TMANet -->
            <tbody>
              <tr onmouseout="tmanet_stop()" onmouseover="tmanet_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='tmanet_image'>
                      <img src='images/tmanet.jpg' width=100%>
                    </div>
                    <img src='images/tmanet.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function tmanet_start() {
                      document.getElementById('tmanet_image').style.opacity = "1";
                    }

                    function tmanet_stop() {
                      document.getElementById('tmanet_image').style.opacity = "0";
                    }
                    tmanet_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://wanghao9610.github.io/TMANet/">
                    <span class="papertitle">TMANet: Temporal Memory Attention for Video Semantic Segmentation</span>
                  </a>
                  <br>
                  <strong>Hao Wang</strong>,
                  <a href="https://scholar.google.com/citations?user=NDPvobAAAAAJ">Weining Wang</a>,
                  <a href="https://scholar.google.com/citations?user=sOI-S7oAAAAJ">Jing Liu</a>,
                  <br>
                  <em>ICIP</em>, 2021
                  <br>
                  <a href="https://wanghao9610.github.io/TMANet/">Project</a>
                  /
                  <a href="https://arxiv.org/abs/2102.08643">Paper</a>
                  /
                  <a href="https://github.com/wanghao9610/TMANet">
                    <img
                      src="https://img.shields.io/github/stars/wanghao9610/TMANet?style=flat-square&logo=github&logoColor=white&label=Code&color=black&cacheSeconds=86400"
                      alt="GitHub stars">
                  </a>
                  <p>
                    A novel self-attention and temporal memory mechanism to capture long-range temporal relations
                    between frames, avoiding the computational cost of optical flow prediction.
                  </p>
                </td>
              </tr>
            </tbody>

            <!-- WL-MSR -->
            <tbody>
              <tr onmouseout="wlmsr_stop()" onmouseover="wlmsr_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='wlmsr_image'>
                      <img src='images/wlmsr.jpg' width=100%>
                    </div>
                    <img src='images/wlmsr.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function wlmsr_start() {
                      document.getElementById('wlmsr_image').style.opacity = "1";
                    }

                    function wlmsr_stop() {
                      document.getElementById('wlmsr_image').style.opacity = "0";
                    }
                    wlmsr_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/10096063">
                    <span class="papertitle">WL-MSR: Watch and Listen for Multimodal Subtitle Recognition</span>
                  </a>
                  <br>
                  <a>Jiawei Liu</a>,
                  <strong>Hao Wang</strong>,
                  <a href="https://scholar.google.com/citations?user=NDPvobAAAAAJ">Weining Wang</a>,
                  <a href="https://scholar.google.com/citations?user=XWunp9YAAAAJ">Xingjian He</a>,
                  <a href="https://scholar.google.com/citations?user=sOI-S7oAAAAJ">Jing Liu</a>,
                  <br>
                  <em>ICASSP</em>, 2023
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/10096063">Paper</a>
                  <p>
                    A framework that fuses OCR and ASR information using a Transformer model with mask/crop strategies
                    and multi-level identity embeddings to generate comprehensive video subtitles.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Experience -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>üíº Experience</h2>
                  <p>
                  <ul>
                    <!-- <strong>2025.01 - Present</strong>
                    <br>
                    Research intern in Meituan M17-MM, co-worked with <a
                      href="https://scholar.google.com/citations?user=3PFZAg0AAAAJ&hl">Limeng Qiao</a> and <a
                      href="https://forestlinma.com/">Lin Ma</a>.
                    <br>
                    <strong>2022.07 - 2025.01</strong>
                    <br>
                    Research intern in Meituan Vision Intelligence Department, co-worked with <a
                      href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl">Zequn Jie</a> and <a
                      href="https://forestlinma.com/">Lin Ma</a>.
                    <br> -->
                    <li><strong>2021.05 - 2021.08</strong>
                      <br>
                      Application research intern in Tencent AI Platform Department.
                    </li>
                    <br>
                    <li><strong>2019.09 - 2020.07</strong>
                      <br>
                      Application project intern in Huawei Photo Processing Department.
                    </li>
                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Education -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>üéì Education</h2>
                  <p>
                  <ul>
                    <li><strong>2022.09 - Present</strong>
                      <br>
                      Ph.D. student in the School of Intelligent Systems Engineering, SYSU, and Pengcheng Laboratory,
                      co-supervised
                      by
                      <a href="https://scholar.google.com/citations?user=voxznZAAAAAJ&hl">Prof. Xiaodan Liang</a> and <a
                        href="https://scholar.google.com/citations?user=c3iwWRcAAAAJ&hl">Associate Prof. Xiangyuan
                        Lan</a>.
                    </li>
                    <br>
                    <li><strong>2019.09 - 2022.06</strong>
                      <br>
                      Master student in the School of Artificial Intelligence, UCAS, and the Institute of Automation,
                      CAS,
                      supervised by <a href="https://scholar.google.com/citations?user=sOI-S7oAAAAJ&hl">Prof. Jing
                        Liu</a>.
                    </li>
                    <br>
                    <li><strong>2015.09 - 2019.06</strong>
                      <br>
                      Bachelor student in the School of Electronic and Information Engineering, BJTU.
                    </li>
                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Services -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>üåà Services</h2>
                  <p>
                  <ul>
                    <li>Reviewer for Conferences: AAAI2026, ICCV2023, ECCV2024.</li>
                    <li>Reviewer for Journals: Proceddings of the IEEE.</li>
                    <!-- <li>Reviewer for journals: .</li> -->
                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Awards -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>üèÜ Awards</h2>
                  <p>
                  <ul>
                    <li>[<strong>2021.09</strong>] 1st place in the 1st <a
                        href="https://www.vspwdataset.com/Leaderboard.html">VSPW Challenge</a> Workshop, ICCV 2021.</li>
                  </ul>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          
          <!-- MapMyVisitors -->
          <script type="text/javascript" id="mapmyvisitors"
            src="//mapmyvisitors.com/map.js?d=nq_TN8mwe6ePYMGkPX8UT8YNMkNICnUTSaVc7okfb5k&cl=ffffff&w=500&co=000000&ct=808080&t=n"></script>

          <!-- Footer -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <p style="text-align:right;font-size:small;margin-bottom:4px;">
                    Template is stolen from <a href="https://jonbarron.info/">Jon Barron's website</a> and modified by
                    me. Feel free to steal this website's <a
                      href="https://github.com/wanghao9610/wanghao9610.github.io">source
                      code</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Counter -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 16px;width:100%;vertical-align:middle">
                  <p style="text-align:right;font-size:14px;color:#666;margin:0;">
                    <i class="fas fa-eye"></i> <span id="busuanzi_value_site_pv"
                      style="color:#4A90E2;font-weight:bold;">-</span> views
                    &nbsp;&nbsp;|&nbsp;&nbsp;
                    <i class="fas fa-user"></i> <span id="busuanzi_value_site_uv"
                      style="color:#4A90E2;font-weight:bold;">-</span> visitors
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

        </td>
      </tr>
  </table>
</body>

</html>